{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset\n",
    "data1 = pd.read_csv('MachineLearningCVE/Monday-WorkingHours.pcap_ISCX.csv')\n",
    "#data2 = pd.read_csv('MachineLearningCVE/Tuesday-WorkingHours.pcap_ISCX.csv')\n",
    "#data3 = pd.read_csv('MachineLearningCVE/Wednesday-workingHours.pcap_ISCX.csv')\n",
    "data4 = pd.read_csv('MachineLearningCVE/Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv')\n",
    "#data5 = pd.read_csv('MachineLearningCVE/Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv')\n",
    "#data6 = pd.read_csv('MachineLearningCVE/Friday-WorkingHours-Morning.pcap_ISCX.csv')\n",
    "data7 = pd.read_csv('MachineLearningCVE/Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv')\n",
    "#data8 = pd.read_csv('MachineLearningCVE/Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data dimensions: \n",
      "Data1 -> 529918 rows, 79 columns\n",
      "Data2 -> 170366 rows, 79 columns\n",
      "Data3 -> 286467 rows, 79 columns\n"
     ]
    }
   ],
   "source": [
    "data_list = [data1,data4,data7]\n",
    "\n",
    "print('Data dimensions: ')\n",
    "for i, data in enumerate(data_list, start = 1):\n",
    "  rows, cols = data.shape\n",
    "  print(f'Data{i} -> {rows} rows, {cols} columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New dimension:\n",
      "Number of rows: 986751\n",
      "Number of columns: 79\n",
      "Total cells: 77953329\n"
     ]
    }
   ],
   "source": [
    "df = pd.concat(data_list)\n",
    "rows, cols = df.shape\n",
    "\n",
    "print('New dimension:')\n",
    "print(f'Number of rows: {rows}')\n",
    "print(f'Number of columns: {cols}')\n",
    "print(f'Total cells: {rows * cols}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleting dataframes after concating to save memory\n",
    "for d in data_list: del d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming the columns by removing leading/trailing whitespace\n",
    "col_names = {col: col.strip() for col in df.columns}\n",
    "df.rename(columns = col_names, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Destination Port', 'Flow Duration', 'Total Fwd Packets',\n",
      "       'Total Backward Packets', 'Total Length of Fwd Packets',\n",
      "       'Total Length of Bwd Packets', 'Fwd Packet Length Max',\n",
      "       'Fwd Packet Length Min', 'Fwd Packet Length Mean',\n",
      "       'Fwd Packet Length Std', 'Bwd Packet Length Max',\n",
      "       'Bwd Packet Length Min', 'Bwd Packet Length Mean',\n",
      "       'Bwd Packet Length Std', 'Flow Bytes/s', 'Flow Packets/s',\n",
      "       'Flow IAT Mean', 'Flow IAT Std', 'Flow IAT Max', 'Flow IAT Min',\n",
      "       'Fwd IAT Total', 'Fwd IAT Mean', 'Fwd IAT Std', 'Fwd IAT Max',\n",
      "       'Fwd IAT Min', 'Bwd IAT Total', 'Bwd IAT Mean', 'Bwd IAT Std',\n",
      "       'Bwd IAT Max', 'Bwd IAT Min', 'Fwd PSH Flags', 'Bwd PSH Flags',\n",
      "       'Fwd URG Flags', 'Bwd URG Flags', 'Fwd Header Length',\n",
      "       'Bwd Header Length', 'Fwd Packets/s', 'Bwd Packets/s',\n",
      "       'Min Packet Length', 'Max Packet Length', 'Packet Length Mean',\n",
      "       'Packet Length Std', 'Packet Length Variance', 'FIN Flag Count',\n",
      "       'SYN Flag Count', 'RST Flag Count', 'PSH Flag Count', 'ACK Flag Count',\n",
      "       'URG Flag Count', 'CWE Flag Count', 'ECE Flag Count', 'Down/Up Ratio',\n",
      "       'Average Packet Size', 'Avg Fwd Segment Size', 'Avg Bwd Segment Size',\n",
      "       'Fwd Header Length.1', 'Fwd Avg Bytes/Bulk', 'Fwd Avg Packets/Bulk',\n",
      "       'Fwd Avg Bulk Rate', 'Bwd Avg Bytes/Bulk', 'Bwd Avg Packets/Bulk',\n",
      "       'Bwd Avg Bulk Rate', 'Subflow Fwd Packets', 'Subflow Fwd Bytes',\n",
      "       'Subflow Bwd Packets', 'Subflow Bwd Bytes', 'Init_Win_bytes_forward',\n",
      "       'Init_Win_bytes_backward', 'act_data_pkt_fwd', 'min_seg_size_forward',\n",
      "       'Active Mean', 'Active Std', 'Active Max', 'Active Min', 'Idle Mean',\n",
      "       'Idle Std', 'Idle Max', 'Idle Min', 'Label'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features from labels\n",
    "X = df.drop(columns=['Label'])\n",
    "y = df['Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features shape: (985808, 78)\n",
      "Unique labels in y: [0 1 2 3 4]\n"
     ]
    }
   ],
   "source": [
    "# Replace ±∞ with NaN\n",
    "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Suppose the dataset has a 'Label' column\n",
    "df['Label'] = df['Label'].astype('category').cat.codes\n",
    "\n",
    "# Separate features & labels\n",
    "X = df.drop(columns=['Label'])\n",
    "y = df['Label'].values\n",
    "\n",
    "print(\"Features shape:\", X.shape)\n",
    "print(\"Unique labels in y:\", np.unique(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (690064, 78) (690064,)\n",
      "Val:   (147872, 78) (147872,)\n",
      "Test:  (147872, 78) (147872,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.15, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "val_ratio = 0.15 / 0.85\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_val, y_train_val, test_size=val_ratio, \n",
    "    random_state=42, stratify=y_train_val\n",
    ")\n",
    "\n",
    "print(\"Train:\", X_train.shape, y_train.shape)\n",
    "print(\"Val:  \", X_val.shape, y_val.shape)\n",
    "print(\"Test: \", X_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Justification for 70-15-15 split:\n",
    "\n",
    "70% Training: Enough data to learn robust patterns.\n",
    "\n",
    "15% Validation: A separate set for hyperparameter tuning (e.g., learning rate, noise multiplier for DP, etc.).\n",
    "\n",
    "15% Test: Kept strictly for final evaluation. This prevents overfitting to the validation set and provides an unbiased measure of performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install torch opacus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from opacus import PrivacyEngine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to PyTorch tensors\n",
    "X_train_t = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_t = torch.tensor(y_train, dtype=torch.long)\n",
    "\n",
    "X_val_t = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val_t = torch.tensor(y_val, dtype=torch.long)\n",
    "\n",
    "X_test_t = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_t = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_t, y_train_t)\n",
    "val_dataset   = TensorDataset(X_val_t,   y_val_t)\n",
    "test_dataset  = TensorDataset(X_test_t,  y_test_t)\n",
    "\n",
    "batch_size = 256\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=batch_size, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=64, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, num_classes)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "num_classes = len(np.unique(y_train))  # If it's binary, likely 2\n",
    "model = SimpleMLP(input_dim, hidden_dim=64, num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\poly\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\opacus\\privacy_engine.py:96: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# DP hyperparameters\n",
    "noise_multiplier = 1.0\n",
    "max_grad_norm = 1.0\n",
    "epochs = 5\n",
    "delta = 1e-5\n",
    "\n",
    "privacy_engine = PrivacyEngine()\n",
    "\n",
    "model, optimizer, train_loader = privacy_engine.make_private(\n",
    "    module=model,\n",
    "    optimizer=optimizer,\n",
    "    data_loader=train_loader,\n",
    "    noise_multiplier=noise_multiplier,\n",
    "    max_grad_norm=max_grad_norm,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\poly\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 0.2518, Acc: 0.9210\n",
      "Epoch [2/5], Loss: 0.0459, Acc: 0.9902\n",
      "Epoch [3/5], Loss: 0.0484, Acc: 0.9918\n",
      "Epoch [4/5], Loss: 0.0488, Acc: 0.9923\n",
      "Epoch [5/5], Loss: 0.0511, Acc: 0.9923\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct, total = 0, 0\n",
    "    \n",
    "    for data, target in train_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * data.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct += (predicted == target).sum().item()\n",
    "        total += target.size(0)\n",
    "    \n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc  = correct / total\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {epoch_loss:.4f}, Acc: {epoch_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9920471759359446\n",
      "Precision: 0.9899076903767561\n",
      "Recall: 0.9920539385414412\n",
      "F1-Score: 0.9909704361100694\n",
      "Confusion Matrix:\n",
      " [[123106    618      0      0      0]\n",
      " [   230  23591      0      0      0]\n",
      " [   226      0      0      0      0]\n",
      " [     3      0      0      0      0]\n",
      " [    98      0      0      0      0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\poly\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import (\n",
    "    confusion_matrix, precision_score, recall_score, f1_score, roc_auc_score\n",
    ")\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    all_probs = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for data, target in loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            outputs = model(data)\n",
    "            \n",
    "            # If multi-class, outputs have shape [batch_size, num_classes]\n",
    "            probs = torch.softmax(outputs, dim=1)[:,1]  # Probability of class=1\n",
    "            all_probs.append(probs.cpu().numpy())\n",
    "            all_labels.append(target.cpu().numpy())\n",
    "            \n",
    "            _, pred = torch.max(outputs, 1)\n",
    "            correct += (pred == target).sum().item()\n",
    "            total += target.size(0)\n",
    "    acc = correct / total\n",
    "    return acc, np.concatenate(all_probs), np.concatenate(all_labels)\n",
    "\n",
    "test_acc, y_prob_test, y_test_true = evaluate(model, test_loader)\n",
    "print(\"Test Accuracy:\", test_acc)\n",
    "\n",
    "y_pred_test = (y_prob_test >= 0.5).astype(int)\n",
    "\n",
    "prec = precision_score(y_test_true, y_pred_test,average='weighted')\n",
    "rec  = recall_score(y_test_true, y_pred_test,average='weighted')\n",
    "f1   = f1_score(y_test_true, y_pred_test,average='weighted')\n",
    "cm   = confusion_matrix(y_test_true, y_pred_test)\n",
    "\n",
    "print(\"Precision:\", prec)\n",
    "print(\"Recall:\", rec)\n",
    "print(\"F1-Score:\", f1)\n",
    "print(\"Confusion Matrix:\\n\", cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ε = 0.20 for δ = 1e-05\n"
     ]
    }
   ],
   "source": [
    "epsilon = privacy_engine.get_epsilon(delta=delta)\n",
    "print(f\"ε = {epsilon:.2f} for δ = {delta}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Round 1/3] Federated Model Test Accuracy: 0.9870\n",
      "[Round 2/3] Federated Model Test Accuracy: 0.9900\n",
      "[Round 3/3] Federated Model Test Accuracy: 0.9913\n",
      "\n",
      "=== Federated Model (with DP) Final Evaluation ===\n",
      "Accuracy:      0.9913\n",
      "Precision:     0.9893\n",
      "Detection Rate (Recall): 0.9914\n",
      "F1-Score:      0.9903\n",
      "Confusion Matrix:\n",
      " [[123006    718      0      0      0]\n",
      " [   230  23591      0      0      0]\n",
      " [   226      0      0      0      0]\n",
      " [     3      0      0      0      0]\n",
      " [    98      0      0      0      0]]\n",
      "\n",
      "=== Comparison: Centralized vs Federated ===\n",
      "Centralized Model:\n",
      "  Accuracy:      0.9920\n",
      "  Recall:        0.9921\n",
      "  Precision:     0.9899\n",
      "  F1-Score:      0.9910\n",
      "Federated Model:\n",
      "  Accuracy:      0.9913\n",
      "  Recall:        0.9914\n",
      "  Precision:     0.9893\n",
      "  F1-Score:      0.9903\n",
      "\n",
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\poly\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "def remove_opacus_prefix(state_dict, prefix=\"_module.\"):\n",
    "    \"\"\"Remove the '_module.' prefix added by Opacus from each key.\"\"\"\n",
    "    new_state_dict = {}\n",
    "    for k, v in state_dict.items():\n",
    "        # If the key starts with \"_module.\", strip it off\n",
    "        if k.startswith(prefix):\n",
    "            k = k[len(prefix):]\n",
    "        new_state_dict[k] = v\n",
    "    return new_state_dict\n",
    "\n",
    "# 2.1 Federated Learning Setup \n",
    "\n",
    "def split_dataset_for_federation(train_dataset, num_clients=3, seed=42):\n",
    "    \"\"\"\n",
    "    Splits the given training dataset into `num_clients` subsets.\n",
    "    Returns a list of Subset objects.\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    data_size = len(train_dataset)\n",
    "    indices   = np.arange(data_size)\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    # Equal (or almost equal) split\n",
    "    split_sizes = [data_size // num_clients] * num_clients\n",
    "    remainder = data_size % num_clients\n",
    "    for i in range(remainder):\n",
    "        split_sizes[i] += 1\n",
    "    \n",
    "    subsets = []\n",
    "    start_idx = 0\n",
    "    for size in split_sizes:\n",
    "        end_idx = start_idx + size\n",
    "        subset_indices = indices[start_idx:end_idx]\n",
    "        subsets.append(Subset(train_dataset, subset_indices))\n",
    "        start_idx = end_idx\n",
    "    return subsets\n",
    "\n",
    "\n",
    "def train_one_client_federated(model, train_data, device, dp_params, local_epochs=1):\n",
    "    local_model = copy.deepcopy(model)\n",
    "    local_model.train()\n",
    "\n",
    "    # Create local dataloader\n",
    "    local_batch_size = 128\n",
    "    local_loader = DataLoader(train_data, batch_size=local_batch_size, shuffle=True)\n",
    "    \n",
    "    # Define local optimizer & loss\n",
    "    optimizer = optim.SGD(local_model.parameters(), lr=dp_params['lr'])\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "\n",
    "    # Attach local PrivacyEngine for user-level DP\n",
    "    privacy_engine_local = PrivacyEngine()\n",
    "    local_model, optimizer, local_loader = privacy_engine_local.make_private(\n",
    "        module=local_model,\n",
    "        optimizer=optimizer,\n",
    "        data_loader=local_loader,\n",
    "        noise_multiplier=dp_params['noise_multiplier'],\n",
    "        max_grad_norm=dp_params['max_grad_norm'],\n",
    "    )\n",
    "    \n",
    "    # Local training\n",
    "    for _ in range(local_epochs):\n",
    "        for data_batch, target_batch in local_loader:\n",
    "            data_batch, target_batch = data_batch.to(device), target_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = local_model(data_batch)\n",
    "            loss = criterion(outputs, target_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    # Remove the \"_module.\" prefix so that we can load it into the non-wrapped model\n",
    "    cleaned_state_dict = remove_opacus_prefix(local_model.state_dict())\n",
    "\n",
    "    # Return the state_dict and number of samples\n",
    "    return cleaned_state_dict, len(train_data)\n",
    "\n",
    "\n",
    "def federated_avg(state_dicts, data_counts):\n",
    "    \"\"\"\n",
    "    Performs Federated Averaging (FedAvg).\n",
    "    state_dicts: list of parameter dictionaries from each client\n",
    "    data_counts: number of samples for each client, to do weighted averaging\n",
    "    \"\"\"\n",
    "    # Keys in the first state_dict\n",
    "    global_model_dict = copy.deepcopy(state_dicts[0])\n",
    "    \n",
    "    total_data_points = sum(data_counts)\n",
    "    \n",
    "    for key in global_model_dict.keys():\n",
    "        # Weighted sum of the parameter across all clients\n",
    "        global_model_dict[key] = sum(\n",
    "            (state_dicts[i][key] * data_counts[i] for i in range(len(state_dicts)))\n",
    "        ) / total_data_points\n",
    "    \n",
    "    return global_model_dict\n",
    "\n",
    "\n",
    "def federated_training(\n",
    "    global_model, \n",
    "    train_dataset,\n",
    "    test_loader, \n",
    "    device, \n",
    "    dp_params,\n",
    "    num_clients=3,\n",
    "    global_rounds=3,\n",
    "    local_epochs=1\n",
    "):\n",
    "    \"\"\"\n",
    "    High-level loop for Federated Training.\n",
    "    - Splits train_dataset among `num_clients`\n",
    "    - For each global round:\n",
    "        - Each client trains locally (with DP)\n",
    "        - We average (FedAvg) all client model weights\n",
    "    - Returns final global model\n",
    "    \"\"\"\n",
    "    # Split dataset\n",
    "    client_subsets = split_dataset_for_federation(train_dataset, num_clients=num_clients)\n",
    "    \n",
    "    # Initialize global model\n",
    "    fed_model = copy.deepcopy(global_model)\n",
    "    fed_model.to(device)\n",
    "    \n",
    "    for round_idx in range(global_rounds):\n",
    "        client_state_dicts = []\n",
    "        client_data_counts = []\n",
    "        \n",
    "        # Broadcast global model to each client; train locally with DP\n",
    "        for client_idx, subset in enumerate(client_subsets):\n",
    "            state_dict, data_count = train_one_client_federated(\n",
    "                fed_model, subset, device, dp_params, local_epochs\n",
    "            )\n",
    "            client_state_dicts.append(state_dict)\n",
    "            client_data_counts.append(data_count)\n",
    "        \n",
    "        # Average the client models into the new global model\n",
    "        new_global_dict = federated_avg(client_state_dicts, client_data_counts)\n",
    "        fed_model.load_state_dict(new_global_dict)\n",
    "        \n",
    "        # Evaluate on test set after each global round\n",
    "        test_acc, y_prob_test, y_test_true = evaluate(fed_model, test_loader)\n",
    "        print(f\"[Round {round_idx+1}/{global_rounds}] Federated Model Test Accuracy: {test_acc:.4f}\")\n",
    "    \n",
    "    return fed_model\n",
    "\n",
    "\n",
    "# Federated Training + Comparison\n",
    "\n",
    "# We will reuse the 'train_dataset' from your code above, but we actually only\n",
    "# have: train_dataset, val_dataset, and test_dataset.\n",
    "#\n",
    "# Here, for demonstration, let's do federated learning on the \"train_dataset\".\n",
    "# If you wish, you could also combine train_dataset + val_dataset for more data.\n",
    "\n",
    "# For convenience, let's combine train_dataset + val_dataset into one big set\n",
    "# for the federated simulation. Then we'll test on the same test set.\n",
    "combined_indices = list(range(len(train_dataset))) + [\n",
    "    len(train_dataset) + i for i in range(len(val_dataset))\n",
    "]\n",
    "combined_dataset = torch.utils.data.ConcatDataset([train_dataset, val_dataset])\n",
    "\n",
    "# We will define new DP parameters for the federated approach\n",
    "federated_dp_params = {\n",
    "    'noise_multiplier': 1.0,\n",
    "    'max_grad_norm': 1.0,\n",
    "    'lr': 0.01,\n",
    "    'delta': 1e-5\n",
    "}\n",
    "\n",
    "# Create a fresh model (same architecture) for federated training\n",
    "federated_model = SimpleMLP(input_dim, hidden_dim=64, num_classes=num_classes)\n",
    "\n",
    "# Number of clients (federated nodes)\n",
    "num_clients = 3\n",
    "# Number of global rounds\n",
    "global_rounds = 3\n",
    "# Local epochs each client trains per round\n",
    "local_epochs = 1\n",
    "\n",
    "# Perform federated training\n",
    "federated_model = federated_training(\n",
    "    global_model=federated_model,\n",
    "    train_dataset=combined_dataset,\n",
    "    test_loader=test_loader,\n",
    "    device=device,\n",
    "    dp_params=federated_dp_params,\n",
    "    num_clients=num_clients,\n",
    "    global_rounds=global_rounds,\n",
    "    local_epochs=local_epochs,\n",
    ")\n",
    "\n",
    "# Final evaluation of the federated model\n",
    "fed_acc, fed_prob_test, fed_test_true = evaluate(federated_model, test_loader)\n",
    "fed_pred_test = (fed_prob_test >= 0.5).astype(int)\n",
    "\n",
    "fed_prec = precision_score(fed_test_true, fed_pred_test, average='weighted')\n",
    "fed_rec  = recall_score(fed_test_true, fed_pred_test, average='weighted')\n",
    "fed_f1   = f1_score(fed_test_true, fed_pred_test, average='weighted')\n",
    "fed_cm   = confusion_matrix(fed_test_true, fed_pred_test)\n",
    "\n",
    "print(\"\\n=== Federated Model (with DP) Final Evaluation ===\")\n",
    "print(f\"Accuracy:      {fed_acc:.4f}\")\n",
    "print(f\"Precision:     {fed_prec:.4f}\")\n",
    "print(f\"Detection Rate (Recall): {fed_rec:.4f}\")\n",
    "print(f\"F1-Score:      {fed_f1:.4f}\")\n",
    "print(\"Confusion Matrix:\\n\", fed_cm)\n",
    "\n",
    "\n",
    "# Compare Federated vs Centralized\n",
    "#\n",
    "# The centralized model metrics were already printed in your original code:\n",
    "#   - test_acc, prec, rec, f1, cm\n",
    "#   - You can reference them to compare with `fed_acc, fed_prec, fed_rec, fed_f1, fed_cm`.\n",
    "#\n",
    "# Example comparison print (uncomment if you want them side by side):\n",
    "print(\"\\n=== Comparison: Centralized vs Federated ===\")\n",
    "print(\"Centralized Model:\")\n",
    "print(f\"  Accuracy:      {test_acc:.4f}\")\n",
    "print(f\"  Recall:        {rec:.4f}\")\n",
    "print(f\"  Precision:     {prec:.4f}\")\n",
    "print(f\"  F1-Score:      {f1:.4f}\")\n",
    "\n",
    "print(\"Federated Model:\")\n",
    "print(f\"  Accuracy:      {fed_acc:.4f}\")\n",
    "print(f\"  Recall:        {fed_rec:.4f}\")\n",
    "print(f\"  Precision:     {fed_prec:.4f}\")\n",
    "print(f\"  F1-Score:      {fed_f1:.4f}\")\n",
    "\n",
    "print(\"\\nDone.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adversarial Threats:\n",
    "\n",
    "Model/Data Poisoning\n",
    "\n",
    "Malicious clients submit crafted updates or train on manipulated data to degrade or backdoor the global model.\n",
    "\n",
    "Inference Attacks\n",
    "\n",
    "Attackers try to recover private information (e.g., membership inference) from aggregated updates.\n",
    "\n",
    "Free-Rider Attacks\n",
    "\n",
    "Malicious participants exploit the system’s resources without contributing meaningful data or updates.\n",
    "\n",
    "\n",
    "-----------------------------\n",
    "\n",
    "\n",
    "Defenses:\n",
    "\n",
    "Differential Privacy\n",
    "\n",
    "Limits per-client leakage by adding noise to gradients.\n",
    "\n",
    "Robust Aggregation\n",
    "\n",
    "Methods like Krum, Bulyan, or Trimmed Mean filter out or downweight outlier (potentially poisoned) updates.\n",
    "\n",
    "Secure Aggregation\n",
    "\n",
    "Cryptographic techniques ensure server only sees aggregated data, protecting individual updates from exposure.\n",
    "\n",
    "Anomaly Detection\n",
    "\n",
    "Screens client updates for suspicious deviations, large gradients, or performance drops on a small validation set.\n",
    "\n",
    "Trusted Execution Environments (TEEs)\n",
    "\n",
    "Protect aggregation and model parameters from tampering at server-level.\n",
    "\n",
    "By combining these measures—especially DP + robust aggregation—you can better secure federated learning against poisoning, privacy leaks, and free-riders while preserving model utility."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
