{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset\n",
    "#data1 = pd.read_csv('MachineLearningCVE/Monday-WorkingHours.pcap_ISCX.csv')\n",
    "data2 = pd.read_csv('MachineLearningCVE/Tuesday-WorkingHours.pcap_ISCX.csv')\n",
    "data3 = pd.read_csv('MachineLearningCVE/Wednesday-workingHours.pcap_ISCX.csv')\n",
    "data4 = pd.read_csv('MachineLearningCVE/Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv')\n",
    "data5 = pd.read_csv('MachineLearningCVE/Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv')\n",
    "#data6 = pd.read_csv('MachineLearningCVE/Friday-WorkingHours-Morning.pcap_ISCX.csv')\n",
    "data7 = pd.read_csv('MachineLearningCVE/Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv')\n",
    "data8 = pd.read_csv('MachineLearningCVE/Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = [data2, data3, data4, data5, data7, data8]\n",
    "\n",
    "print('Data dimensions: ')\n",
    "for i, data in enumerate(data_list, start = 1):\n",
    "  rows, cols = data.shape\n",
    "  print(f'Data{i} -> {rows} rows, {cols} columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(data_list)\n",
    "rows, cols = df.shape\n",
    "\n",
    "print('New dimension:')\n",
    "print(f'Number of rows: {rows}')\n",
    "print(f'Number of columns: {cols}')\n",
    "print(f'Total cells: {rows * cols}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleting dataframes after concating to save memory\n",
    "for d in data_list: del d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming the columns by removing leading/trailing whitespace\n",
    "col_names = {col: col.strip() for col in df.columns}\n",
    "df.rename(columns = col_names, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features from labels\n",
    "X = df.drop(columns=['Label'])\n",
    "y = df['Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_splits = 100\n",
    "split_size = len(df) // num_splits\n",
    "\n",
    "dfs = []\n",
    "start = 0\n",
    "for i in range(num_splits):\n",
    "    end = start + split_size\n",
    "    df_slice = df.iloc[start:end].copy()  # copy the slice to avoid reindexing issues\n",
    "\n",
    "    # Replace infinities in the slice\n",
    "    df_slice.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "    # Drop or fill NaNs\n",
    "    df_slice.dropna(inplace=True)\n",
    "\n",
    "    dfs.append(df_slice)\n",
    "    start = end\n",
    "\n",
    "# Last slice for any leftover rows\n",
    "if end < len(df):\n",
    "    df_slice = df.iloc[end:].copy()\n",
    "    df_slice.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    df_slice.dropna(inplace=True)\n",
    "    dfs.append(df_slice)\n",
    "\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "print(\"DataFrame shape after cleaning:\", df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace +∞, -∞ with NaN\n",
    "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Drop rows that have become NaN\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Now separate X and y again\n",
    "X = df.drop(columns=['Label'])\n",
    "y = df['Label']\n",
    "\n",
    "# Then you can do your scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 70% train, 15% val, 15% test\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "    X_scaled, y, \n",
    "    test_size=0.15, \n",
    "    stratify=y, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Now split train_val into train and validation\n",
    "val_ratio = 0.15 / 0.85  # roughly 0.1765\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_val, y_train_val,\n",
    "    test_size=val_ratio, \n",
    "    stratify=y_train_val, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"Training set shape:\", X_train.shape, y_train.shape)\n",
    "print(\"Validation set shape:\", X_val.shape, y_val.shape)\n",
    "print(\"Test set shape:\", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Justification for 70-15-15 split:\n",
    "\n",
    "70% Training: Enough data to learn robust patterns.\n",
    "\n",
    "15% Validation: A separate set for hyperparameter tuning (e.g., learning rate, noise multiplier for DP, etc.).\n",
    "\n",
    "15% Test: Kept strictly for final evaluation. This prevents overfitting to the validation set and provides an unbiased measure of performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install tensorflow-privacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from tensorflow_privacy.privacy.optimizers.dp_optimizer_keras import DPKerasSGDOptimizer\n",
    "from tensorflow_privacy.privacy.analysis import compute_dp_sgd_privacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate   = 0.01\n",
    "batch_size      = 256\n",
    "microbatch_size = 64\n",
    "epochs          = 5\n",
    "noise_multiplier = 1.1\n",
    "l2_norm_clip    = 1.0\n",
    "delta           = 1e-5  # Typical DP delta\n",
    "\n",
    "def create_model(input_dim):\n",
    "    model = keras.Sequential([\n",
    "        layers.Dense(64, activation='relu', input_shape=(input_dim,)),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(1, activation='sigmoid')  # for binary classification\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "model = create_model(X_train.shape[1])\n",
    "\n",
    "dp_optimizer = DPKerasSGDOptimizer(\n",
    "    l2_norm_clip=l2_norm_clip,\n",
    "    noise_multiplier=noise_multiplier,\n",
    "    num_microbatches=microbatch_size,\n",
    "    learning_rate=learning_rate\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=dp_optimizer,\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=epochs,\n",
    "    validation_data=(X_val, y_val),\n",
    "    batch_size=batch_size,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Compute approximate epsilon\n",
    "eps, _ = compute_dp_sgd_privacy.compute_dp_sgd_privacy(\n",
    "    n=len(X_train),\n",
    "    batch_size=batch_size,\n",
    "    noise_multiplier=noise_multiplier,\n",
    "    epochs=epochs,\n",
    "    delta=delta\n",
    ")\n",
    "print(f\"For delta={delta}, the estimated epsilon is: {eps:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    confusion_matrix, precision_score, recall_score, f1_score, \n",
    "    accuracy_score, roc_auc_score\n",
    ")\n",
    "\n",
    "y_prob = model.predict(X_test).ravel()\n",
    "y_pred = (y_prob >= 0.5).astype(int)\n",
    "\n",
    "acc  = accuracy_score(y_test, y_pred)\n",
    "prec = precision_score(y_test, y_pred, average='binary')\n",
    "rec  = recall_score(y_test, y_pred, average='binary')\n",
    "f1   = f1_score(y_test, y_pred, average='binary')\n",
    "auc  = roc_auc_score(y_test, y_prob)\n",
    "\n",
    "print(\"Accuracy:\", acc)\n",
    "print(\"Precision:\", prec)\n",
    "print(\"Recall:\", rec)\n",
    "print(\"F1-Score:\", f1)\n",
    "print(\"AUC-ROC:\", auc)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\\n\", cm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretation of Confusion Matrix\n",
    "\n",
    "TP: Attack traffic correctly labeled “attack”.\n",
    "\n",
    "TN: Benign traffic correctly labeled “benign”.\n",
    "\n",
    "FP: Benign mislabeled as attack (false alarms).\n",
    "\n",
    "FN: Attacks mislabeled as benign (missed detections).\n",
    "\n",
    "Privacy (ε) vs. Accuracy Trade-Off\n",
    "\n",
    "A higher noise_multiplier → better privacy (lower ε) but generally lower accuracy.\n",
    "\n",
    "A lower noise_multiplier → weaker privacy (larger ε) but higher accuracy.\n",
    "\n",
    "Goal: Find a balance that meets your organization’s regulatory/privacy needs while maintaining high detection performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLIENTS = 5\n",
    "client_data = []\n",
    "total_samples = len(X_train)\n",
    "samples_per_client = total_samples // NUM_CLIENTS\n",
    "\n",
    "start = 0\n",
    "for i in range(NUM_CLIENTS):\n",
    "    end = start + samples_per_client\n",
    "    X_c = X_train[start:end]\n",
    "    y_c = y_train[start:end]\n",
    "    client_data.append((X_c, y_c))\n",
    "    start = end\n",
    "\n",
    "# You might have leftover samples if total_samples is not divisible by NUM_CLIENTS.\n",
    "# Those leftover samples can go to the last client or be distributed among them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install tensorflow_federated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_federated as tff\n",
    "import tensorflow as tf\n",
    "\n",
    "def create_tf_dataset_for_client(data_tuple):\n",
    "    X_c, y_c = data_tuple\n",
    "    ds = tf.data.Dataset.from_tensor_slices((X_c, y_c))\n",
    "    ds = ds.shuffle(buffer_size=100).batch(32)\n",
    "    return ds\n",
    "\n",
    "federated_train_data = [create_tf_dataset_for_client(cd) for cd in client_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn():\n",
    "    keras_model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=(X_train.shape[1],)),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    return tff.learning.from_keras_model(\n",
    "        keras_model,\n",
    "        input_spec=federated_train_data[0].element_spec,\n",
    "        loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "        metrics=[tf.keras.metrics.BinaryAccuracy()]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterative_process = tff.learning.build_federated_averaging_process(model_fn)\n",
    "state = iterative_process.initialize()\n",
    "\n",
    "NUM_ROUNDS = 5\n",
    "for round_num in range(1, NUM_ROUNDS+1):\n",
    "    state, metrics = iterative_process.next(state, federated_train_data)\n",
    "    print(f\"Round {round_num} metrics: {metrics}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_federated.python.aggregators import differential_privacy\n",
    "\n",
    "dp_factory = differential_privacy.DifferentiallyPrivateFactory.gaussian_fixed(\n",
    "    noise_multiplier=1.0,\n",
    "    clients_per_round=NUM_CLIENTS,\n",
    "    clipped_count_budget=10,  # or as desired\n",
    "    l2_norm_clip=1.0\n",
    ")\n",
    "\n",
    "dp_iterative_process = tff.learning.build_federated_averaging_process(\n",
    "    model_fn,\n",
    "    model_aggregator=dp_factory.create(\n",
    "        value_type=iterative_process.get_model_weights(state).trainable\n",
    "    )\n",
    ")\n",
    "\n",
    "dp_state = dp_iterative_process.initialize()\n",
    "\n",
    "for round_num in range(1, NUM_ROUNDS+1):\n",
    "    dp_state, dp_metrics = dp_iterative_process.next(dp_state, federated_train_data)\n",
    "    print(f\"Round {round_num} DP metrics: {dp_metrics}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "federated_weights = dp_iterative_process.get_model_weights(dp_state)\n",
    "model_fed = create_model(X_train.shape[1])  # same architecture as model_fn\n",
    "federated_weights.assign_weights_to(model_fed)\n",
    "\n",
    "y_prob_fed = model_fed.predict(X_test).ravel()\n",
    "y_pred_fed = (y_prob_fed >= 0.5).astype(int)\n",
    "\n",
    "acc_fed = accuracy_score(y_test, y_pred_fed)\n",
    "print(\"Federated model accuracy on test set:\", acc_fed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison with Centralized Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2 Adversarial Analysis and Defense\n",
    "\n",
    "Model Poisoning: A malicious client uploads gradients that skew or degrade the model.\n",
    "\n",
    "Defense: Use robust aggregators (e.g., multi-Krum, outlier detection, or differential privacy** with clipping**).\n",
    "\n",
    "Inference Attacks: Attackers try to infer details about a client’s data from global model parameters.\n",
    "\n",
    "Defense: User-level differential privacy ensures that updates are clipped and noised, reducing the potential for inference.\n",
    "\n",
    "Backdoor Attacks: Attackers embed triggers in the model.\n",
    "\n",
    "Defense: Periodic validation against known safe data, outlier detection in gradient updates, or specialized defensive techniques (e.g., verifiable model updates)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
